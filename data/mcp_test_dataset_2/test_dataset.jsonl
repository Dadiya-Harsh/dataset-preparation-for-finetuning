{"prompt": "### Instruction:\nImplement Python code for a file named 'postgres_server.py' from the 'MCP-DEMO' repository\n\n### Response:", "response": "import os\n\nimport psycopg2\nfrom dotenv import load_dotenv\nfrom mcp.server.fastmcp import FastMCP\nfrom psycopg2.extras import RealDictCursor\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize FastMCP server\nmcp = FastMCP(\n    name=\"postgres_server\",\n    port=8001,\n    host=\"localhost\",\n    description=\"Access PostgresSQL databases by name and perform SQL operations\"\n)\n\n# Database connection pool\ndb_connections = {}\n\n@mcp.resource(name=\"postgres_server\", description=\"Provides access to PostgresSQL database in localhost.\", uri=os.getenv(\"DATABASE_URI\"))\ndef postgres_server():\n    \"\"\"\n    This resource establishes a connection to the specified PostgresSQL database.\n\n    Returns:\n        dict: Connection status information\n    \"\"\"\n    return {\"status\": \"ready\", \"message\": \"PostgresSQL connection resource is ready\"}\n\n@mcp.prompt(\n    name=\"postgres_query_prompt\",\n    description=\"Prompt for executing a SQL query on the PostgresSQL database.\"\n)\ndef postgres_query_prompt(db_name, query=None):\n    \"\"\"\n    Provides guidance and context for executing SQL queries.\n\n    Args:\n        db_name: The name of the database to query        : Optional query to execute\n\n    Returns:\n        list: A list of chat messages for the AI to consider\n        :param db_name:\n        :param query:\n    \"\"\"\n    # If query is provided, include it in the prompt\n    query_context = f\"\\nQuery to execute: {query}\" if query else \"\"\n\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"I need to work with the PostgresSQL database named '{db_name}'.{query_context}\n\nPlease help me with the following:\n\n1. Connect to the database '{db_name}' using the connect_database tool\n2. If I provided a query, execute it using the execute_query tool\n3. If I didn't provide a query, help me explore the database by listing tables\n4. Format the results in a clear, readable way\n5. Provide brief explanations of what the data represents (if applicable)\n6. Suggest possible follow-up queries based on the schema or results\n\nRemember to use parameterized queries for any values that come from user input to prevent SQL injection.\n\"\"\"\n        }\n    ]\n\n@mcp.prompt(\n    name=\"analyze_table_prompt\",\n    description=\"Prompt for analyzing the structure and content of a database table.\"\n)\ndef analyze_table_prompt(db_name, table_name):\n    \"\"\"\n    Provides a prompt for analyzing a specific database table.\n\n    Args:\n        db_name: The name of the database\n        table_name: The name of the table to analyze\n\n    Returns:\n        list: A list of chat messages for the AI to consider\n    \"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"I need to analyze the table '{table_name}' in the PostgresSQL database '{db_name}'.\n\nPlease help me with the following:\n\n1. Connect to the database '{db_name}' using the connect_database tool\n2. Describe the structure of the '{table_name}' table using the describe_table tool\n3. Get a sample of the data (first 5-10 rows) using execute_query\n4. Analyze the table structure and provide insights on:\n   - Primary keys and relationships\n   - Data types and constraints\n   - Potential indexing opportunities\n   - Any anomalies in the schema design\n5. Summarize what this table appears to represent in the database\n6. Suggest some useful queries I might want to run on this table\n\nFormat the results in a clear, readable way with appropriate markdown formatting.\n\"\"\"\n        }\n    ]\n\n@mcp.tool(name=\"connect_database\", description=\"Connect to a specific PostgresSQL database.\")\ndef connect_database(db_name):\n    \"\"\"\n    Connects to a specific PostgresSQL database.\n\n    Args:\n        db_name (str): Name of the database to connect to\n\n    Returns:\n        dict: Connection status information\n    \"\"\"\n    if not db_name:\n        return {\"error\": \"Database name is required\"}\n\n    try:\n        # Get connection string from environment variables\n        base_uri = os.getenv(\"DATABASE_URI\")\n        if not base_uri:\n            return {\"error\": \"DATABASE_URI environment variable not set\"}\n\n        # Create a connection to the specified database\n        connection = psycopg2.connect(f\"{base_uri}/{db_name}\")\n\n        # Store the connection in our pool\n        db_connections[db_name] = connection\n\n        return {\"status\": \"success\", \"message\": f\"Connected to database '{db_name}'\"}\n\n    except Exception as e:\n        return {\"error\": str(e)}\n\n@mcp.tool(name=\"execute_query\", description=\"Executes SQL queries on the connected PostgresSQL database.\")\ndef execute_query(db_name, query, params=None):\n    \"\"\"\n    Executes SQL queries on the specified database.\n\n    Args:\n        db_name (str): Name of the database to query         (str): SQL query to execute\n        params (list, optional): Parameters for the query\n\n    Returns:\n        dict: Query results or error information\n        :param db_name:\n        :param params:\n        :param query:\n    \"\"\"\n    if not db_name:\n        return {\"error\": \"Database name is required\"}\n\n    if not query:\n        return {\"error\": \"Query is required\"}\n\n    if db_name not in db_connections:\n        # Try to connect if not already connected\n        connection_result = connect_database(db_name)\n        if \"error\" in connection_result:\n            return connection_result\n\n    connection = db_connections[db_name]\n\n    try:\n        with connection.cursor(cursor_factory=RealDictCursor) as cursor:\n            cursor.execute(query, params or [])\n\n            # For SELECT queries, return the results\n            if query.strip().upper().startswith(\"SELECT\"):\n                results = cursor.fetchall()\n                # Convert results to a list of dictionaries\n                return {\"results\": [dict(row) for row in results]}\n\n            # For other queries (INSERT, UPDATE, DELETE), commit and return row count\n            else:\n                connection.commit()\n                return {\"status\": \"success\", \"rows_affected\": cursor.rowcount}\n\n    except Exception as e:\n        connection.rollback()  # Rollback in case of error\n        return {\"error\": str(e)}\n\n@mcp.tool(name=\"list_tables\", description=\"Lists all tables in the specified database.\")\ndef list_tables(db_name):\n    \"\"\"\n    Lists all tables in the specified database.\n\n    Args:\n        db_name (str): Name of the database\n\n    Returns:\n        dict: List of tables or error information\n    \"\"\"\n    if not db_name:\n        return {\"error\": \"Database name is required\"}\n\n    query = \"\"\"\n    SELECT table_name \n    FROM information_schema.tables \n    WHERE table_schema = 'public'\n    ORDER BY table_name;\n    \"\"\"\n\n    return execute_query(db_name, query)\n\n@mcp.tool(name=\"describe_table\", description=\"Describes the structure of a specific table.\")\ndef describe_table(db_name, table_name):\n    \"\"\"\n    Describes the structure of a specific table.\n\n    Args:\n        db_name (str): Name of the database\n        table_name (str): Name of the table to describe\n\n    Returns:\n        dict: Table structure information or error\n    \"\"\"\n    if not db_name or not table_name:\n        return {\"error\": \"Database name and table name are required\"}\n\n    query = \"\"\"\n    SELECT \n        column_name, \n        data_type, \n        is_nullable,\n        column_default\n    FROM \n        information_schema.columns\n    WHERE \n        table_schema = 'public' AND \n        table_name = %s\n    ORDER BY \n        ordinal_position;\n    \"\"\"\n\n    return execute_query(db_name, query, [table_name])\n\n@mcp.tool(name=\"close_connection\", description=\"Closes the connection to the specified database.\")\ndef close_connection(db_name):\n    \"\"\"\n    Closes the connection to the specified database.\n\n    Args:\n        db_name (str): Name of the database connection to close\n\n    Returns:\n        dict: Operation status\n    \"\"\"\n    if not db_name:\n        return {\"error\": \"Database name is required\"}\n\n    if db_name not in db_connections:\n        return {\"error\": f\"No active connection to database '{db_name}'\"}\n\n    try:\n        db_connections[db_name].close()\n        del db_connections[db_name]\n        return {\"status\": \"success\", \"message\": f\"Connection to '{db_name}' closed\"}\n\n    except Exception as e:\n        return {\"error\": str(e)}\n\nif __name__ == \"__main__\":\n    print(f\"Starting MCP PostgresSQL server on http://localhost:5000\")\n    mcp.run(transport=\"sse\")", "metadata": {"source": "github", "repo": "Dadiya-Harsh-wappnet/MCP-DEMO", "path": "learning/Servers/postgres_server.py", "language": "Python", "stars": 1}}
{"prompt": "### Instruction:\nImplement Python code for a file named 'web_search_using_tavily.py' from the 'MCP-DEMO' repository\n\n### Response:", "response": "from mcp.server.fastmcp import FastMCP, Context\nfrom mcp.types import TextContent\nfrom tavily import TavilyClient\nimport os\nfrom dotenv import load_dotenv\nimport json\n\nload_dotenv()\n\nmcp = FastMCP(\n    name=\"web_search_server\",\n    host=\"localhost\",\n    port=8000,\n    log_level=\"INFO\",\n)\n\n@mcp.tool(\n    name=\"web_search\",\n    description=\"Perform a web search using the Tavily API.\"\n)\nasync def web_search(query: str, ctx: Context) -> list[TextContent]:\n    \"\"\"\n    Perform a web search using the Tavily API.\n\n    Args:\n        query: The search query string.\n        ctx: Context for logging and progress reporting.\n\n    Returns:\n        List[TextContent]: JSON-encoded search results.\n    \"\"\"\n    await ctx.info(f\"Performing web search for query: {query}\")\n    try:\n        tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n        await ctx.report_progress(50, 100)\n        response = tavily_client.search(query)\n        await ctx.report_progress(100, 100)\n\n        # Format results as JSON\n        results = [\n            {\n                \"title\": result[\"title\"],\n                \"url\": result[\"url\"],\n                \"content\": result[\"content\"]\n            } for result in response[\"results\"]\n        ]\n\n        return [TextContent(\n            type=\"text\",\n            text=json.dumps({\n                \"status\": \"success\",\n                \"query\": query,\n                \"results\": results\n            })\n        )]\n    except Exception as e:\n        await ctx.error(f\"Error performing web search: {str(e)}\")\n        return [TextContent(\n            type=\"text\",\n            text=json.dumps({\n                \"status\": \"error\",\n                \"message\": str(e)\n            })\n        )]\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"sse\")", "metadata": {"source": "github", "repo": "Dadiya-Harsh-wappnet/MCP-DEMO", "path": "learning/Servers/web_search_using_tavily.py", "language": "Python", "stars": 1}}
{"prompt": "### Instruction:\nImplement Python code for a file named 'type_dict_ex.py' from the 'MCP-DEMO' repository\n\n### Response:", "response": "from typing import Annotated, TypedDict, Callable, get_type_hints\n\n\nname = Annotated[str, \"Name of the person\"]\n\nage = Annotated[int, \"Age of the person\"]   \n\nclass Person(TypedDict):\n    name: name\n    age: age\n\nprint(get_type_hints(Person))  # {'name': <class 'str'>, 'age': <class 'int'>} \nprint(name.__metadata__)  # (str, 'Name of the person')", "metadata": {"source": "github", "repo": "Dadiya-Harsh-wappnet/MCP-DEMO", "path": "learning/annotations/type_dict_ex.py", "language": "Python", "stars": 1}}
{"prompt": "### Instruction:\nImplement Python code for a file named 'TaskGroup_usage.py' from the 'MCP-DEMO' repository\n\n### Response:", "response": "import asyncio\n\nasync def fetch_data(id_number: int, delay: int) -> str:\n    \"\"\"\n    Simulate a network call to fetch data\n    :param:\n        id_number: id of the task\n        delay: time to wait before returning data\n    :return:\n        string that contains how much time it took to fetch data\n    \"\"\"\n\n    print(f\"Task {id_number}: Getting data...\")\n    await asyncio.sleep(delay)\n    print(f\"Task {id_number}: Data received!\")\n    return f\"Task {id_number}: Data after {delay} seconds\"\n\nasync def main() -> None:\n    \"\"\"\n    Main function to run the async tasks\n    :return:\n    \"\"\"\n\n    print(\"learned about TaskGroup function!\")\n\n    tasks = []\n\n    \"\"\"\n    Create a TaskGroup to manage multiple tasks and,\n    we are using async with statement which is used to create a context manager for the TaskGroup.\n    This ensures that all tasks are completed before exiting the context.\n    \"\"\"\n    async with asyncio.TaskGroup() as tg:\n        for i, sleep_time in enumerate([2, 3, 4], start=1):\n            tasks.append(tg.create_task(fetch_data(i, sleep_time)))\n\n    results = [task.result() for task in tasks]\n\n\n    for results in results:\n        print(f\"Received results: {results}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n    # print(f\"Main Function coroutine object: {main()}\")\n    #\n    # print(f\"fetch_data Function coroutine object: {fetch_data(5)}\")", "metadata": {"source": "github", "repo": "Dadiya-Harsh-wappnet/MCP-DEMO", "path": "learning/asyncoronus python/TaskGroup_usage.py", "language": "Python", "stars": 1}}
{"prompt": "### Instruction:\nImplement Python code for a file named 'async_1.py' from the 'MCP-DEMO' repository\n\n### Response:", "response": "import asyncio\n\nasync def main() -> None:\n    \"\"\"\n    whenever async function is called, it returns a coroutine object\n    :return:\n    \"\"\"\n\n    print(\"Hello from mcp-demo!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())", "metadata": {"source": "github", "repo": "Dadiya-Harsh-wappnet/MCP-DEMO", "path": "learning/asyncoronus python/async_1.py", "language": "Python", "stars": 1}}
{"prompt": "### Instruction:\nImplement Python code for a file named 'await_1.py' from the 'MCP-DEMO' repository\n\n### Response:", "response": "import asyncio\n\nasync def fetch_data(delay: int) -> str:\n    print(\"Getting data...\")\n\n    await asyncio.sleep(delay)\n    print(\"Data received!\")\n    return f\"Data after {delay} seconds\"\n\nasync def main() -> None:\n    print(\"learned about await keyword!\")\n    data = fetch_data(2)\n    print(\"fetch_data function is called\")\n\n    real_data = await data\n    print(real_data)\n    print(\"fetch_data function is awaited\")\n    # loop = asyncio.get_event_loop()\n    # data = loop.run_until_complete(fetch_data(2))\n    # print(data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n    # print(f\"Main Function coroutine object: {main()}\")\n    #\n    # print(f\"fetch_data Function coroutine object: {fetch_data(5)}\")", "metadata": {"source": "github", "repo": "Dadiya-Harsh-wappnet/MCP-DEMO", "path": "learning/asyncoronus python/await_1.py", "language": "Python", "stars": 1}}
{"prompt": "### Instruction:\nImplement Python code for a file named 'couroutine_seprately.py' from the 'MCP-DEMO' repository\n\n### Response:", "response": "import asyncio\n\nasync def fetch_data(id_number: int, delay: int) -> str:\n    \"\"\"\n    Simulate a network call to fetch data\n    :param:\n        id_number: id of the task\n        delay: time to wait before returning data\n    :return:\n        string that contains how much time it took to fetch data\n    \"\"\"\n\n    print(f\"Task {id_number}: Getting data...\")\n    await asyncio.sleep(delay)\n    print(f\"Task {id_number}: Data received!\")\n    return f\"Task {id_number}: Data after {delay} seconds\"\n\nasync def main() -> None:\n    \"\"\"\n    Main function to run the async tasks\n    :return:\n    \"\"\"\n\n    print(\"learned about await keyword!\")\n\n    # Create a list of tasks\n\n    data_1 = fetch_data(1, 2)\n    print(await data_1)\n    print(\"fetch_data function is called for task 1\")\n\n    data_2 = fetch_data(2, 3)\n    print(await data_2)\n    print(\"fetch_data function is called for task 2\")\n\n    data_3 = fetch_data(3, 1)\n    print(await data_3)\n    print(\"fetch_data function is called for task 3\")\n\n    # Run the tasks concurrently\n    # results = await asyncio.gather(*tasks)\n\n    # for result in results:\n    #     print(result)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n    # print(f\"Main Function coroutine object: {main()}\")\n    #\n    # print(f\"fetch_data Function coroutine object: {fetch_data(5)}\")", "metadata": {"source": "github", "repo": "Dadiya-Harsh-wappnet/MCP-DEMO", "path": "learning/asyncoronus python/couroutine_seprately.py", "language": "Python", "stars": 1}}
{"prompt": "### Instruction:\nImplement Python code for a file named 'create_task.py' from the 'MCP-DEMO' repository\n\n### Response:", "response": "import asyncio\nimport time\nasync def fetch_data(id_number: int, delay: int) -> str:\n    \"\"\"\n    Simulate a network call to fetch data\n    :param:\n        id_number: id of the task\n        delay: time to wait before returning data\n    :return:\n        string that contains how much time it took to fetch data\n    \"\"\"\n\n    print(f\"Task {id_number}: Getting data...\")\n    await asyncio.sleep(delay)\n    print(f\"Task {id_number}: Data received!\")\n    return f\"Task {id_number}: Data received after {delay} seconds\"\n\nasync def main() -> None:\n    \"\"\"\n    Main function to run the async tasks\n    :return:\n    \"\"\"\n    print(\"learned about create_task() function!\")\n    task_1 = asyncio.create_task(fetch_data(1, 2))\n    task_2 = asyncio.create_task(fetch_data(2, 3))\n    task_3 = asyncio.create_task(fetch_data(3, 1))\n\n    # Run the tasks one by one\n    result_1 = await task_1\n    print(result_1)\n    result_2 = await task_2\n    print(result_2)\n    result_3 = await task_3\n    print(result_3)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\n", "metadata": {"source": "github", "repo": "Dadiya-Harsh-wappnet/MCP-DEMO", "path": "learning/asyncoronus python/create_task.py", "language": "Python", "stars": 1}}
{"prompt": "### Instruction:\nImplement Python code for a file named 'gather_task.py' from the 'MCP-DEMO' repository\n\n### Response:", "response": "import asyncio\n\nasync def fetch_data(id_number: int, delay: int) -> str:\n    \"\"\"\n    Simulate a network call to fetch data\n    :param:\n        id_number: id of the task\n        delay: time to wait before returning data\n    :return:\n        string that contains how much time it took to fetch data\n    \"\"\"\n\n    print(f\"Task {id_number}: Getting data...\")\n    await asyncio.sleep(delay)\n    print(f\"Task {id_number}: Data received!\")\n    return f\"Task {id_number}: Data after {delay} seconds\"\n\nasync def main() -> None:\n    \"\"\"\n    Main function to run the async tasks\n    :return:\n    \"\"\"\n\n    print(\"learned about await keyword!\")\n\n    # Create a list of tasks\n    tasks = [\n        fetch_data(1, 2),\n        fetch_data(2, 3),\n        fetch_data(3, 1)\n    ]\n\n    # Run the tasks concurrently\n    results = await asyncio.gather(*tasks)\n    \"\"\"\n    Note:\n        if we are using asyncio.gather(), we should remember it is not good at error handling, so use it carefully.\n        If one of the tasks or coroutine fails, it will not cancel other tasks or coroutine.\n    \"\"\"\n\n    for result in results:\n        print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())", "metadata": {"source": "github", "repo": "Dadiya-Harsh-wappnet/MCP-DEMO", "path": "learning/asyncoronus python/gather_task.py", "language": "Python", "stars": 1}}
{"prompt": "### Instruction:\nImplement Python code for a file named 'server.py' from the 'MCP-DEMO' repository\n\n### Response:", "response": "import json\nimport os\nfrom mcp.server.fastmcp import FastMCP\nimport nest_asyncio\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nnest_asyncio.apply()\n\nmcp = FastMCP(name = \"LLM-Server\", host = \"0.0.0.0\", port = 8050)\n\n\n@mcp.tool()\ndef knowledge_base() -> str:\n    \"\"\"Retrieve the entire knowledge base as a formatted string.\n\n    Returns:\n        A formatted string containing all Q&A pairs from the knowledge base.\n    \"\"\"\n    try:\n        kb_path = os.path.join(os.path.dirname(__file__), \"data\", \"kb.json\")\n        with open(kb_path, \"r\") as f:\n            kb_data = json.load(f)\n\n        # Format the knowledge base as a string\n        kb_text = \"Here is the retrieved knowledge base:\\n\\n\"\n\n        if isinstance(kb_data, list):\n            for i, item in enumerate(kb_data, 1):\n                if isinstance(item, dict):\n                    question = item.get(\"question\", \"Unknown question\")\n                    answer = item.get(\"answer\", \"Unknown answer\")\n                else:\n                    question = f\"Item {i}\"\n                    answer = str(item)\n\n                kb_text += f\"Q{i}: {question}\\n\"\n                kb_text += f\"A{i}: {answer}\\n\\n\"\n        else:\n            kb_text += f\"Knowledge base content: {json.dumps(kb_data, indent=2)}\\n\\n\"\n\n        return kb_text\n    except FileNotFoundError:\n        return \"Error: Knowledge base file not found\"\n    except json.JSONDecodeError:\n        return \"Error: Invalid JSON in knowledge base file\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n\n\nif __name__ == \"__main__\":\n    if os.getenv(\"TRANSPORT\") == \"stdio\":\n        print(\"Running server with stdio transport\")\n        mcp.run(transport=\"stdio\")\n    elif os.getenv(\"TRANSPORT\") == \"sse\":\n        print(\"Running server with sse transport\")\n        mcp.run(transport=\"sse\")\n    # mcp.run(transport=\"stdio\")\n    # mcp.run(transport=\"sse\")", "metadata": {"source": "github", "repo": "Dadiya-Harsh-wappnet/MCP-DEMO", "path": "learning/groq/server.py", "language": "Python", "stars": 1}}
{"prompt": "### Instruction:\nImplement Python code for a file named 'simple_async_groq_client.py' from the 'MCP-DEMO' repository\n\n### Response:", "response": "import asyncio\nimport json\nfrom contextlib import AsyncExitStack\nimport os\nfrom typing import Any, Dict, List, Optional\n\nimport nest_asyncio\nfrom dotenv import load_dotenv\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n# from openai import AsyncOpenAI\nfrom groq import AsyncGroq\n\nfrom mcp.client.session import ClientSession\n# Apply nest_asyncio to allow nested event loops (needed for Jupyter/IPython)\nnest_asyncio.apply()\n\n# Load environment variables\nload_dotenv()\n\n\nclass MCPGroqClient:\n    \"\"\"Client for interacting with groq models using MCP tools.\"\"\"\n\n    def __init__(self, model: str = \"llama-3.3-70b-versatile\"):\n        \"\"\"Initialize the groq MCP client.\n\n        Args:\n            model: The groq model to use.\n        \"\"\"\n        # Initialize session and client objects\n        self.session: Optional[ClientSession] = None\n        self.exit_stack = AsyncExitStack()\n        self.groq_client = AsyncGroq(api_key=os.getenv(\"GROQ_API_KEY\"))\n        self.model = model\n        self.stdio: Optional[Any] = None\n        self.write: Optional[Any] = None\n\n    async def connect_to_server(self, server_script_path: str = \"server.py\"):\n        \"\"\"Connect to an MCP server.\n\n        Args:\n            server_script_path: Path to the server script.\n        \"\"\"\n        # Server configuration\n        server_params = StdioServerParameters(\n            command=\"python\",\n            args=[server_script_path],\n        )\n\n        # Connect to the server\n        stdio_transport = await self.exit_stack.enter_async_context(\n            stdio_client(server_params)\n        )\n        self.stdio, self.write = stdio_transport\n        self.session = await self.exit_stack.enter_async_context(\n            ClientSession(self.stdio, self.write)\n        )\n\n        # Initialize the connection\n        await self.session.initialize()\n\n        # List available tools\n        tools_result = await self.session.list_tools()\n        print(\"\\nConnected to server with tools:\")\n        for tool in tools_result.tools:\n            print(f\"  - {tool.name}: {tool.description}\")\n\n    async def get_mcp_tools(self) -> List[Dict[str, Any]]:\n        \"\"\"Get available tools from the MCP server in groq format.\n\n        Returns:\n            A list of tools in groq format.\n        \"\"\"\n        tools_result = await self.session.list_tools()\n        return [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool.name,\n                    \"description\": tool.description,\n                    \"parameters\": tool.inputSchema,\n                },\n            }\n            for tool in tools_result.tools\n        ]\n\n    async def process_query(self, query: str) -> str:\n        \"\"\"Process a query using groq and available MCP tools.\n\n        Args:\n            query: The user query.\n\n        Returns:\n            The response from groq.\n        \"\"\"\n        # Get available tools\n        tools = await self.get_mcp_tools()\n\n        # Initial groq API call\n        response = await self.groq_client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": query}],\n            tools=tools,\n            tool_choice=\"auto\",\n        )\n\n        # Get assistant's response\n        assistant_message = response.choices[0].message\n\n        # Initialize conversation with user query and assistant response\n        messages = [\n            {\"role\": \"user\", \"content\": query},\n            assistant_message,\n        ]\n\n        # Handle tool calls if present\n        if assistant_message.tool_calls:\n            # Process each tool call\n            for tool_call in assistant_message.tool_calls:\n                # Execute tool call\n                result = await self.session.call_tool(\n                    tool_call.function.name,\n                    arguments=json.loads(tool_call.function.arguments),\n                )\n\n                # Add tool response to conversation\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call.id,\n                        \"content\": result.content[0].text,\n                    }\n                )\n\n            # Get final response from groq with tool results\n            final_response = await self.groq_client.chat.completions.create(\n                model=self.model,\n                messages=messages,\n                tools=tools,\n                tool_choice=\"none\",  # Don't allow more tool calls\n            )\n\n            return final_response.choices[0].message.content\n\n        # No tool calls, just return the direct response\n        return assistant_message.content\n\n    async def cleanup(self):\n        \"\"\"Clean up resources.\"\"\"\n        await self.exit_stack.aclose()\n\n\nasync def main():\n    \"\"\"Main entry point for the client.\"\"\"\n    client = MCPGroqClient()\n    await client.connect_to_server()\n\n    # Example: Ask about company vacation policy\n    query = \"What is our company's vacation policy?\"\n    print(f\"\\nQuery: {query}\")\n\n    response = await client.process_query(query)\n    print(f\"\\nResponse: {response}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())", "metadata": {"source": "github", "repo": "Dadiya-Harsh-wappnet/MCP-DEMO", "path": "learning/groq/simple_async_groq_client.py", "language": "Python", "stars": 1}}
{"prompt": "### Instruction:\nImplement Python code for a file named 'simple_groq_client.py' from the 'MCP-DEMO' repository\n\n### Response:", "response": "import json\nfrom typing import Any, Dict, List, Optional\n\nfrom groq import Groq\nimport os\nfrom dotenv import load_dotenv\nfrom typing import Optional, Any\nfrom contextlib import AsyncExitStack\nimport asyncio\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n\nload_dotenv()\n\nclass SimpleGroqClient:\n    \"\"\"Client for interacting with groq models using MCP tools.\"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the groq client.\n        \"\"\"\n        self.stdio = None\n        self.groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n        self.model = os.getenv(\"GROQ_MODEL\", \"llama-3.3-70b-versatile\")\n        self.session: Optional[ClientSession] = None\n        self.exit_stack = AsyncExitStack()\n        self.stdio: Optional[Any] = None\n        self.write: Optional[Any] = None\n\n    async def connect_to_server(self, server_script_path: str):\n        \"\"\"Connect to an MCP server\n\n        Args:\n            server_script_path: Path to the server script (.py or .js)\n        \"\"\"\n        is_python = server_script_path.endswith('.py')\n        is_js = server_script_path.endswith('.js')\n        if not (is_python or is_js):\n            raise ValueError(\"Server script must be a .py or .js file\")\n\n        command = \"python\" if is_python else \"node\"\n        server_params = StdioServerParameters(\n            command=command,\n            args=[server_script_path],\n            env=None\n        )\n\n        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))\n        self.stdio, self.write = stdio_transport\n        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))\n\n        await self.session.initialize()\n\n        # List available tools\n        response = await self.session.list_tools()\n        tools = response.tools\n        print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n\n    async def get_mcp_tools(self) -> List[Dict[str, Any]]:\n        \"\"\"Get available tools from the MCP server in groq format.\n\n        Returns:\n            A list of tools in groq format.\n        \"\"\"\n        tools_result = await self.session.list_tools()\n        return [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool.name,\n                    \"description\": tool.description,\n                    \"parameters\": tool.inputSchema,\n                },\n            }\n            for tool in tools_result.tools\n        ]\n\n    async def process_query(self, query: str) -> str:\n        \"\"\"Process a query using groq and available MCP tools.\n\n        Args:\n            query: The user query.\n\n        Returns:\n            The response from groq.\n        \"\"\"\n        # Get available tools\n        tools = await self.get_mcp_tools()\n\n        # Initial groq API call\n        response = self.groq_client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": query}],\n            tools=tools,\n            tool_choice=\"auto\",\n        )\n\n        # Get assistant's response\n        assistant_message = response.choices[0].message\n\n        # Initialize conversation with user query and assistant response\n        messages = [\n            {\"role\": \"user\", \"content\": query},\n            assistant_message,\n        ]\n\n        # Handle tool calls if present\n        if assistant_message.tool_calls:\n            # Process each tool call\n            for tool_call in assistant_message.tool_calls:\n                # Execute tool call\n                result = await self.session.call_tool(\n                    tool_call.function.name,\n                    arguments=json.loads(tool_call.function.arguments),\n                )\n\n                # Add tool response to conversation\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call.id,\n                        \"content\": result.content[0].text,\n                    }\n                )\n\n            # Get final response from groq with tool results\n            final_response = self.groq_client.chat.completions.create(\n                model=self.model,\n                messages=messages,\n                tools=tools,\n                tool_choice=\"none\",  # Don't allow more tool calls\n            )\n\n            return final_response.choices[0].message.content\n\n        # No tool calls, just return the direct response\n        return assistant_message.content\n\n    async def chat_loop(self):\n        \"\"\"Run an interactive chat loop\"\"\"\n        print(\"\\nMCP Client Started!\")\n        print(\"Type your queries or 'quit' to exit.\")\n\n        while True:\n            try:\n                query = input(\"\\nQuery: \").strip()\n\n                if query.lower() == 'quit':\n                    break\n\n                response = await self.process_query(query)\n                print(\"\\n\" + response)\n\n            except Exception as e:\n                print(f\"\\nError: {str(e)}\")\n\n    async def cleanup(self):\n        \"\"\"Clean up resources\"\"\"\n        await self.exit_stack.aclose()\n\n\nasync def main():\n    if len(sys.argv) < 2:\n        print(\"Usage: python client.py <path_to_server_script>\")\n        sys.exit(1)\n\n    client = SimpleGroqClient()\n    try:\n        await client.connect_to_server(sys.argv[1])\n        await client.chat_loop()\n    finally:\n        await client.cleanup()\n\nif __name__ == \"__main__\":\n    import sys\n    asyncio.run(main())", "metadata": {"source": "github", "repo": "Dadiya-Harsh-wappnet/MCP-DEMO", "path": "learning/groq/simple_groq_client.py", "language": "Python", "stars": 1}}
{"prompt": "### Instruction:\nImplement Python code for a file named 'simple_sse_client.py' from the 'MCP-DEMO' repository\n\n### Response:", "response": "import json\nfrom typing import Any, Dict, List, Optional\nfrom contextlib import AsyncExitStack\nimport asyncio\nimport logging\nfrom groq import Groq\nimport os\nfrom dotenv import load_dotenv\nfrom mcp import ClientSession\nfrom mcp.client.sse import sse_client  # Use SSE transport\n\nload_dotenv()\n\n# Configure logging for debugging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass SimpleGroqClient:\n    \"\"\"Client for interacting with Groq models using MCP tools over SSE transport.\"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the Groq client.\n        \"\"\"\n        self.groq_client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n        self.model = os.getenv(\"GROQ_MODEL\", \"llama-3.3-70b-versatile\")\n        self.session: Optional[ClientSession] = None\n        self.exit_stack = AsyncExitStack()\n        self.read_stream: Optional[Any] = None\n        self.write_stream: Optional[Any] = None\n\n    async def connect_to_server(self, server_url: str = \"http://localhost:8002/sse\"):\n        \"\"\"\n        Connect to an MCP server using SSE transport.\n\n        Args:\n            server_url: URL of the MCP server's SSE endpoint (e.g., \"http://localhost:8002/sse\")\n        \"\"\"\n        logger.info(f\"Connecting to MCP server at {server_url}\")\n        try:\n            # Use sse_client from mcp.client.sse\n            sse_transport = await self.exit_stack.enter_async_context(\n                sse_client(\n                    url=server_url,\n                    headers={\"Accept\": \"text/event-stream\"},\n                    timeout=5.0,  # HTTP timeout for initial connection\n                    sse_read_timeout=300.0,  # 5 minutes for SSE read timeout\n                )\n            )\n            self.read_stream, self.write_stream = sse_transport\n\n            # Initialize ClientSession with SSE streams\n            self.session = await self.exit_stack.enter_async_context(\n                ClientSession(self.read_stream, self.write_stream)\n            )\n            await self.session.initialize()\n\n            # List available tools\n            response = await self.session.list_tools()\n            tools = response.tools\n            logger.info(\"Connected to server with tools: %s\", [tool.name for tool in tools])\n            print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n        except Exception as e:\n            logger.error(f\"Failed to connect to server: {str(e)}\")\n            raise\n\n    async def get_mcp_tools(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Get available tools from the MCP server in Groq format.\n\n        Returns:\n            A list of tools in Groq format.\n        \"\"\"\n        if not self.session:\n            raise ValueError(\"Client is not connected to a server\")\n\n        tools_result = await self.session.list_tools()\n        return [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool.name,\n                    \"description\": tool.description,\n                    \"parameters\": tool.inputSchema,\n                },\n            }\n            for tool in tools_result.tools\n        ]\n\n    async def process_query(self, query: str) -> str:\n        \"\"\"\n        Process a query using Groq and available MCP tools.\n\n        Args:\n            query: The user query.\n\n        Returns:\n            The response from Groq.\n        \"\"\"\n        if not self.session:\n            raise ValueError(\"Client is not connected to a server\")\n\n        # Get available tools\n        tools = await self.get_mcp_tools()\n\n        # Initial Groq API call\n        response = self.groq_client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": query}],\n            tools=tools,\n            tool_choice=\"auto\",\n        )\n\n        # Get assistant's response\n        assistant_message = response.choices[0].message\n\n        # Initialize conversation with user query and assistant response\n        messages = [\n            {\"role\": \"user\", \"content\": query},\n            {\n                \"role\": assistant_message.role,\n                \"content\": assistant_message.content,\n                \"tool_calls\": [\n                    {\n                        \"id\": tc.id,\n                        \"type\": \"function\",\n                        \"function\": {\n                            \"name\": tc.function.name,\n                            \"arguments\": tc.function.arguments\n                        }\n                    } for tc in (assistant_message.tool_calls or [])\n                ]\n            }\n        ]\n\n        # Handle tool calls if present\n        if assistant_message.tool_calls:\n            # Process each tool call\n            for tool_call in assistant_message.tool_calls:\n                logger.info(f\"Calling tool: {tool_call.function.name}\")\n                try:\n                    # Execute tool call\n                    result = await self.session.call_tool(\n                        tool_call.function.name,\n                        arguments=json.loads(tool_call.function.arguments),\n                    )\n                    # Add tool response to conversation\n                    messages.append(\n                        {\n                            \"role\": \"tool\",\n                            \"tool_call_id\": tool_call.id,\n                            \"content\": result.content[0].text,\n                        }\n                    )\n                except Exception as e:\n                    logger.error(f\"Error calling tool {tool_call.function.name}: {str(e)}\")\n                    messages.append(\n                        {\n                            \"role\": \"tool\",\n                            \"tool_call_id\": tool_call.id,\n                            \"content\": json.dumps({\"status\": \"error\", \"message\": str(e)}),\n                        }\n                    )\n\n            # Get final response from Groq with tool results\n            final_response = self.groq_client.chat.completions.create(\n                model=self.model,\n                messages=messages,\n                tools=tools,\n                tool_choice=\"none\",  # Don't allow more tool calls\n            )\n\n            return final_response.choices[0].message.content\n\n        # No tool calls, just return the direct response\n        return assistant_message.content\n\n    async def chat_loop(self):\n        \"\"\"\n        Run an interactive chat loop.\n        \"\"\"\n        print(\"\\nMCP Client Started!\")\n        print(\"Type your queries or 'quit' to exit.\")\n\n        while True:\n            try:\n                query = input(\"\\nQuery: \").strip()\n\n                if query.lower() == 'quit':\n                    break\n\n                response = await self.process_query(query)\n                print(\"\\n\" + response)\n\n            except Exception as e:\n                logger.error(f\"Error processing query: {str(e)}\")\n                print(f\"\\nError: {str(e)}\")\n\n    async def cleanup(self):\n        \"\"\"\n        Clean up resources.\n        \"\"\"\n        await self.exit_stack.aclose()\n\n\nasync def main():\n    import sys\n    if len(sys.argv) < 2:\n        server_url = \"http://localhost:8002/sse\"  # Default URL\n        print(f\"Using default server URL: {server_url}\")\n    else:\n        server_url = sys.argv[1]\n        print(f\"Using server URL: {server_url}\")\n\n    client = SimpleGroqClient()\n    try:\n        await client.connect_to_server(server_url)\n        await client.chat_loop()\n    except Exception as e:\n        logger.error(f\"Main loop error: {str(e)}\")\n        print(f\"Error: {str(e)}\")\n    finally:\n        await client.cleanup()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())", "metadata": {"source": "github", "repo": "Dadiya-Harsh-wappnet/MCP-DEMO", "path": "learning/groq/simple_sse_client.py", "language": "Python", "stars": 1}}
{"prompt": "### Instruction:\nImplement Python code for a file named 'tool_usage_example.py' from the 'MCP-DEMO' repository\n\n### Response:", "response": "import json\nfrom groq import Groq\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Initialize Groq client\nclient = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\nmodel = \"llama-3.3-70b-versatile\"\n\n\n# Define weather tools\n\ndef calculate(expression):\n    \"\"\"Tool to evaluate a mathematical expression\"\"\"\n    try:\n        result = eval(expression)\n        return json.dumps({\"result\": result})\n    except:\n        return json.dumps({\"error\": \"Invalid expression\"})\n\n\ndef get_temperature(location: str):\n    # This is a mock tool/function. In a real scenario, you would call a weather API.\n    temperatures = {\"New York\": \"22\u00b0C\", \"London\": \"18\u00b0C\", \"Tokyo\": \"26\u00b0C\", \"Sydney\": \"20\u00b0C\"}\n    return temperatures.get(location, \"Temperature data not available\")\n\n\ndef get_weather_condition(location: str):\n    # This is a mock tool/function. In a real scenario, you would call a weather API.\n    conditions = {\"New York\": \"Sunny\", \"London\": \"Rainy\", \"Tokyo\": \"Cloudy\", \"Sydney\": \"Clear\"}\n    return conditions.get(location, \"Weather condition data not available\")\n\n\n# Define system messages and tools\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant and solve my every query.\"},\n    {\"role\": \"user\",\n     \"content\": \"What's the weather and temperature like in New York and London? Respond with one sentence for each city. Use tools to get the information and what is 10 * 4 + 5. and also tell me which tools are used for every query \"},\n]\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_temperature\",\n            \"description\": \"Get the temperature for a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The name of the city\",\n                    }\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather_condition\",\n            \"description\": \"Get the weather condition for a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The name of the city\",\n                    }\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"calculate\",\n            \"description\": \"Evaluate a mathematical expression\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"expression\": {\n                        \"type\": \"string\",\n                        \"description\": \"The mathematical expression to evaluate\",\n                    }\n                },\n                \"required\": [\"expression\"],\n            },\n        }\n    }\n]\n\n# Make the initial request\nresponse = client.chat.completions.create(\n    model=model, messages=messages, tools=tools, tool_choice=\"auto\", max_completion_tokens=4096, temperature=0.5\n)\n\nresponse_message = response.choices[0].message\ntool_calls = response_message.tool_calls\n\n# Process tool calls\nmessages.append(response_message)\n\navailable_functions = {\n    \"get_temperature\": get_temperature,\n    \"get_weather_condition\": get_weather_condition,\n    \"calculate\": calculate,\n}\n\nfor tool_call in tool_calls:\n    function_name = tool_call.function.name\n    function_to_call = available_functions[function_name]\n    function_args = json.loads(tool_call.function.arguments)\n    function_response = function_to_call(**function_args)\n\n    messages.append(\n        {\n            \"role\": \"tool\",\n            \"content\": str(function_response),\n            \"tool_call_id\": tool_call.id,\n        }\n    )\n\n# Make the final request with tool call results\nfinal_response = client.chat.completions.create(\n    model=model, messages=messages, tools=tools, tool_choice=\"auto\", max_completion_tokens=4096\n)\n\nprint(final_response.choices[0].message.content)\n\n# \"\"\"\n# Output:\n#     (mcp-demo) PS E:\\Projects\\MCP\\mcp-demo\\learning\\groq> uv run .\\client.py\n#         The weather in New York is sunny and the temperature is 22\u00b0C. The weather in London is rainy and the temperature is 18\u00b0C. The result of the mathematical expression 10 * 4 + 5 is 45.\n\n#         The tools used for these queries are:\n#         - get_weather_condition: to get the weather condition for a given location\n#         - get_temperature: to get the temperature for a given location\n#         - calculate: to evaluate a mathematical expression\n# \"\"\"\n", "metadata": {"source": "github", "repo": "Dadiya-Harsh-wappnet/MCP-DEMO", "path": "learning/groq/tool_usage_example.py", "language": "Python", "stars": 1}}
{"prompt": "### Instruction:\nImplement Python code for a file named 'agent.py' from the 'MCP-DEMO' repository\n\n### Response:", "response": "from langgraph.graph import StateGraph, START, END\nfrom typing import TypedDict\nfrom langchain_groq import ChatGroq\nfrom dotenv import load_dotenv\nimport os\nfrom typing import Dict, Any, Annotated\nfrom langgraph.prebuilt import create_react_agent\n\nload_dotenv()\ngroq_api_key = os.getenv(\"GROQ_API_KEY\")\nprint(groq_api_key)\n\nllm = ChatGroq(\n    api_key=groq_api_key,\n    model=\"llama-3.3-70b-versatile\",\n    temperature=0.0\n)\nclass InputState(TypedDict):\n    user_input: Annotated[str, \"The input from the user.\"]\n\nclass IntermediateState(TypedDict):\n    user_input: Annotated[str, \"The input from the user.\"]\n    intermediate_output: Annotated[str, \"The intermediate output.\"]\n\nclass OutputState(TypedDict):\n    user_input: Annotated[str, \"The input from the user.\"]\n    agent_response: Annotated[str, \"The response from the agent.\"]\n    graph_output: Annotated[str, \"The final output.\"]\n\ndef input_node(state: InputState) -> IntermediateState:\n    # This node does not modify the state, but it can be used to read from it if needed.\n    print(f\"Input node received state: {state['user_input']}\")\n\n    return{\n        \"user_input\": state[\"user_input\"],\n        \"intermediate_output\": f\"Processing: {state['user_input']}\"\n    }\n\ndef process_node(state: IntermediateState) -> OutputState:\n    # This node processes the input state and modifies it.\n    user_input = state[\"user_input\"]\n\n    response = llm.invoke(user_input)\n    agent_response = response.content\n\n    return {\n        \"user_input\": user_input,\n        \"agent_response\": agent_response,\n        \"graph_output\": f\"Final output: {agent_response}\"\n    }\n\ndef output_node(state: OutputState) -> OutputState:\n    # This node generates the output state based on the input state.\n    final_output = f\"Final Response: state['graph_output']\"\n\n    return {\n        \"user_input\": state[\"user_input\"],\n        \"agent_response\": state[\"agent_response\"],\n        \"graph_output\": final_output\n    }\n\n\ngraph = StateGraph(InputState, output=OutputState)\ngraph.add_node(\"input_node\", input_node)\ngraph.add_node(\"process_node\", process_node)\ngraph.add_node(\"output_node\", output_node)\ngraph.add_edge(START, \"input_node\")\ngraph.add_edge(\"input_node\", \"process_node\")\ngraph.add_edge(\"process_node\", \"output_node\")\ngraph.add_edge(\"output_node\", END)\n\ncompile = graph.compile()\ncompile.invoke({\"user_input\": \"Hello, world!\"})\n\n", "metadata": {"source": "github", "repo": "Dadiya-Harsh-wappnet/MCP-DEMO", "path": "learning/langgraph/agent.py", "language": "Python", "stars": 1}}
